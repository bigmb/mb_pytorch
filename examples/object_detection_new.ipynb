{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test models \n",
    "import nest_asyncio\n",
    "import warnings\n",
    "from mb_utils.src.logging import logger\n",
    "\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.9'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mb_pytorch.utils.version import version\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fri 01:36:44,027 INF Read data from yaml file: [{'data': {'file': {'root': '/home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dat\n"
     ]
    }
   ],
   "source": [
    "from mb_pytorch.dataloader.loader import DataLoader\n",
    "\n",
    "k = DataLoader('../scripts/detection/object_detection.yaml',logger=logger)\n",
    "k_data = k.data_dict['data']\n",
    "model_data_dict = k.data_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_pytorch.training.train_params import train_helper\n",
    "model_yaml_data = k.data_dict['model']\n",
    "loss_attr,optimizer_attr,optimizer_dict,scheduler_attr,scheduler_dict = train_helper(model_yaml_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "path_logs = os.path.join(os.path.split(k_data['file']['root'])[0], 'logs')\n",
    "writer = SummaryWriter(log_dir=path_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300row [00:00, 109435.66row/s]\n",
      "Fri 01:36:44,096 INF Loaded dataframe from /home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv usin\n",
      "Fri 01:36:44,099 INF Data file: {'root': '/home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv', 've\n",
      "Fri 01:36:44,101 INF Data columns: Index(['image_id', 'image_path', 'bbox', 'label', 'image_type'], dtype='object')                  \n",
      "Fri 01:36:44,102 INF Data will be split into train and validation according to train_file input : True                               \n",
      "Fri 01:36:44,104 INF If unnamed columns are present, they will be removed.                                                           \n",
      "Fri 01:36:44,106 INF If duplicate rows are present, they will be removed.                                                            \n",
      "Fri 01:36:44,113 INF Length of data after removing invalid paths: 300                                                                \n",
      "Fri 01:36:44,115 INF Skipping image verification                                                                                     \n",
      "Fri 01:36:44,117 INF Checking duplicates for the columns: ['image_path']                                                             \n",
      "Fri 01:36:44,119 INF No duplicates found                                                                                             \n",
      "Fri 01:36:44,120 INF Removing unnamed columns                                                                                        \n",
      "Fri 01:36:44,122 INF Columns : Index(['image_id', 'image_path', 'bbox', 'label', 'image_type',\n",
      "       'img_path_check'],\n",
      "      dtype=\n",
      "Fri 01:36:44,129 INF Length of data after removing duplicates and unnamed columns: 240                                               \n",
      "300row [00:00, 177524.15row/s]\n",
      "Fri 01:36:44,138 INF Loaded dataframe from /home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv usin\n",
      "Fri 01:36:44,141 INF Data file: {'root': '/home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv', 've\n",
      "Fri 01:36:44,142 INF Data columns: Index(['image_id', 'image_path', 'bbox', 'label', 'image_type'], dtype='object')                  \n",
      "Fri 01:36:44,144 INF Data will be split into train and validation according to train_file input : False                              \n",
      "Fri 01:36:44,145 INF If unnamed columns are present, they will be removed.                                                           \n",
      "Fri 01:36:44,147 INF If duplicate rows are present, they will be removed.                                                            \n",
      "Fri 01:36:44,152 INF Length of data after removing invalid paths: 300                                                                \n",
      "Fri 01:36:44,153 INF Skipping image verification                                                                                     \n",
      "Fri 01:36:44,155 INF Checking duplicates for the columns: ['image_path']                                                             \n",
      "Fri 01:36:44,157 INF No duplicates found                                                                                             \n",
      "Fri 01:36:44,158 INF Removing unnamed columns                                                                                        \n",
      "Fri 01:36:44,160 INF Columns : Index(['image_id', 'image_path', 'bbox', 'label', 'image_type',\n",
      "       'img_path_check'],\n",
      "      dtype=\n",
      "Fri 01:36:44,163 INF Length of data after removing duplicates and unnamed columns: 60                                                \n"
     ]
    }
   ],
   "source": [
    "train_loader,val_loader,train_dataset,test_dataset = k.data_load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]),\n",
       " {'bbox': tensor([ 17,  66, 402, 556], dtype=torch.int32), 'label': [1]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.2431, 0.2473, 0.2510,  ..., 0.3059, 0.2815, 0.2683],\n",
       "          [0.2301, 0.2387, 0.2437,  ..., 0.3059, 0.2839, 0.2720],\n",
       "          [0.2055, 0.2172, 0.2240,  ..., 0.3094, 0.2853, 0.2723],\n",
       "          ...,\n",
       "          [0.2452, 0.0142, 0.1149,  ..., 0.2067, 0.2212, 0.2017],\n",
       "          [0.2719, 0.0462, 0.0008,  ..., 0.1792, 0.2091, 0.2041],\n",
       "          [0.4482, 0.1613, 0.0118,  ..., 0.1353, 0.1759, 0.1720]],\n",
       " \n",
       "         [[0.4566, 0.4669, 0.4804,  ..., 0.4510, 0.4241, 0.4095],\n",
       "          [0.4391, 0.4523, 0.4658,  ..., 0.4510, 0.4264, 0.4132],\n",
       "          [0.4107, 0.4269, 0.4421,  ..., 0.4545, 0.4346, 0.4239],\n",
       "          ...,\n",
       "          [0.4343, 0.1640, 0.2984,  ..., 0.3366, 0.3476, 0.3281],\n",
       "          [0.4902, 0.2387, 0.1324,  ..., 0.3132, 0.3435, 0.3385],\n",
       "          [0.6723, 0.3395, 0.1471,  ..., 0.2784, 0.3249, 0.3210]],\n",
       " \n",
       "         [[0.3193, 0.3297, 0.3412,  ..., 0.3490, 0.3272, 0.3154],\n",
       "          [0.3076, 0.3177, 0.3320,  ..., 0.3490, 0.3295, 0.3191],\n",
       "          [0.2893, 0.3006, 0.3175,  ..., 0.3525, 0.3332, 0.3228],\n",
       "          ...,\n",
       "          [0.2196, 0.0088, 0.1096,  ..., 0.2425, 0.2534, 0.2340],\n",
       "          [0.2395, 0.0310, 0.0009,  ..., 0.2181, 0.2480, 0.2428],\n",
       "          [0.4216, 0.1462, 0.0137,  ..., 0.1706, 0.2101, 0.2034]]]),\n",
       " {'bbox': tensor([189,  64, 602, 523], dtype=torch.int32), 'label': [2]})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id,data in enumerate(train_loader):\n",
    "#     print(id)\n",
    "#     image,bbox,labels = data.values()\n",
    "#     print(image.shape)\n",
    "#     print(bbox)\n",
    "#     print(labels)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fri 01:37:59,239 INF Model fasterrcnn_resnet50_fpn loaded from torchvision.models.                                                   \n"
     ]
    }
   ],
   "source": [
    "from mb_pytorch.models.modelloader import ModelLoader\n",
    "model_data_load = ModelLoader(k.data_dict['model'])\n",
    "model =  model_data_load.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_pytorch.detection.training import detection_train_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue 18:23:17,965 INF Training loop Starting                                                                                          \n",
      "Tue 18:23:18,451 INF Model fasterrcnn_resnet50_fpn loaded from torchvision.models.                                                   \n",
      "Tue 18:23:18,453 INF Model Loaded                                                                                                    \n",
      "300row [00:00, 165238.50row/s]\n",
      "Tue 18:23:18,457 INF Loaded dataframe from /home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv usin\n",
      "Tue 18:23:18,459 INF Data file: {'root': '/home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv', 've\n",
      "Tue 18:23:18,460 INF Data columns: Index(['image_id', 'image_path', 'bbox', 'label', 'image_type'], dtype='object')                  \n",
      "Tue 18:23:18,462 INF Data will be split into train and validation according to train_file input : True                               \n",
      "Tue 18:23:18,463 INF If unnamed columns are present, they will be removed.                                                           \n",
      "Tue 18:23:18,464 INF If duplicate rows are present, they will be removed.                                                            \n",
      "Tue 18:23:18,468 INF Length of data after removing invalid paths: 300                                                                \n",
      "Tue 18:23:18,469 INF Skipping image verification                                                                                     \n",
      "Tue 18:23:18,471 INF Checking duplicates for the columns: ['image_path']                                                             \n",
      "Tue 18:23:18,472 INF No duplicates found                                                                                             \n",
      "Tue 18:23:18,473 INF Removing unnamed columns                                                                                        \n",
      "Tue 18:23:18,475 INF Columns : Index(['image_id', 'image_path', 'bbox', 'label', 'image_type',\n",
      "       'img_path_check'],\n",
      "      dtype=\n",
      "Tue 18:23:18,481 INF Length of data after removing duplicates and unnamed columns: 240                                               \n",
      "300row [00:00, 306079.10row/s]\n",
      "Tue 18:23:18,485 INF Loaded dataframe from /home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv usin\n",
      "Tue 18:23:18,487 INF Data file: {'root': '/home/malav/Desktop/mb_packages/mb_pytorch/data/fruit_dataset/fruit_dataset_final.csv', 've\n",
      "Tue 18:23:18,488 INF Data columns: Index(['image_id', 'image_path', 'bbox', 'label', 'image_type'], dtype='object')                  \n",
      "Tue 18:23:18,489 INF Data will be split into train and validation according to train_file input : False                              \n",
      "Tue 18:23:18,490 INF If unnamed columns are present, they will be removed.                                                           \n",
      "Tue 18:23:18,491 INF If duplicate rows are present, they will be removed.                                                            \n",
      "Tue 18:23:18,495 INF Length of data after removing invalid paths: 300                                                                \n",
      "Tue 18:23:18,496 INF Skipping image verification                                                                                     \n",
      "Tue 18:23:18,498 INF Checking duplicates for the columns: ['image_path']                                                             \n",
      "Tue 18:23:18,499 INF No duplicates found                                                                                             \n",
      "Tue 18:23:18,501 INF Removing unnamed columns                                                                                        \n",
      "Tue 18:23:18,502 INF Columns : Index(['image_id', 'image_path', 'bbox', 'label', 'image_type',\n",
      "       'img_path_check'],\n",
      "      dtype=\n",
      "Tue 18:23:18,504 INF Length of data after removing duplicates and unnamed columns: 60                                                \n",
      "Tue 18:23:18,507 INF Optimizer and Scheduler Loaded                                                                                  \n",
      "Tue 18:23:18,508 INF Loss: <function cross_entropy at 0x7aae369a4cc0>                                                                \n",
      "Tue 18:23:18,509 INF Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.999]\n",
      "    capturable: False\n",
      "    differe\n",
      "Tue 18:23:18,510 INF Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7aade44f7110>                                           \n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Tue 18:23:18,513 INF Training Started                                                                                                \n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetection_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py:70\u001b[0m, in \u001b[0;36mdetection_train_loop\u001b[0;34m(k_yaml, scheduler, writer, logger, gradcam, gradcam_rgb, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: b,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: label} \u001b[38;5;28;01mfor\u001b[39;00m b,label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(bbox, labels)]      \n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 70\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     73\u001b[0m losses\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:67\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m boxes \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(boxes, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExpected target boxes to be a tensor of shape [N, 4], got \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected target boxes to be of type Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/torch/__init__.py:1559\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m-> 1559\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([4])."
     ]
    }
   ],
   "source": [
    "detection_train_loop(k,model_data_dict,writer=writer,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "for batch_idx, data in enumerate(train_loader):\n",
    "        images,bbox,labels = data.values()\n",
    "        images_l = list(image.to(device) for image in images)\n",
    "        bbox_l = list(b.to(device) for b in bbox)\n",
    "        labels_l = list(label.to(device) for label in labels)  \n",
    "        targets_l = [{'boxes': b,'labels': label} for b,label in zip(bbox, labels)]      \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 600, 800])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_new = [{'boxes': b,'labels': label} for b,label in zip(bbox, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 600, 800])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_l[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([ 17,  66, 402, 556], dtype=torch.int32),\n",
       " 'labels': tensor([1])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_l[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([ 17,  66, 402, 556], dtype=torch.int32),\n",
       "  'labels': tensor([1])},\n",
       " {'boxes': tensor([ 18,   0, 521, 178], dtype=torch.int32),\n",
       "  'labels': tensor([2])},\n",
       " {'boxes': tensor([137, 160, 704, 561], dtype=torch.int32),\n",
       "  'labels': tensor([0])},\n",
       " {'boxes': tensor([ 99,  99, 891, 691], dtype=torch.int32),\n",
       "  'labels': tensor([0])}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
