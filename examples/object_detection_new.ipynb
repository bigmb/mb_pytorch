{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test models \n",
    "import nest_asyncio\n",
    "import warnings\n",
    "from mb_utils.src.logging import logger\n",
    "\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.50'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mb_pytorch.utils.version import version\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mon 02:40:50,528 INF Read data from yaml file: [{'data': {'from_datasets': 'CelebA', 'datasets_params_train': {'root': '/home/malav/D\n",
      "Mon 02:40:50,530 INF Data folder already exists. Using existing data folder :  /home/malav/Desktop/mb_packages/mb_pytorch/data/      \n"
     ]
    }
   ],
   "source": [
    "from mb_pytorch.dataloader.loader import DataLoader\n",
    "\n",
    "k = DataLoader('../scripts/detection/object_detection.yaml',logger=logger)\n",
    "k_data = k.data_dict['data']\n",
    "model_data_dict = k.data_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_pytorch.training.train_params import train_helper\n",
    "model_yaml_data = k.data_dict['model']\n",
    "loss_attr,optimizer_attr,optimizer_dict,scheduler_attr,scheduler_dict = train_helper(model_yaml_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "path_logs = os.path.join(k_data['datasets_params_train']['root'], 'logs')\n",
    "writer = SummaryWriter(log_dir=path_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mon 02:40:58,648 INF Data file: CelebA loading from torchvision.datasets.                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader,val_loader,_,_ = k.data_load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image is : torch.Size([4, 3, 218, 178])\n",
      "target is : [tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1]]), tensor([[ 95,  71, 226, 313],\n",
      "        [ 72,  94, 221, 306],\n",
      "        [216,  59,  91, 126],\n",
      "        [622, 257, 564, 781]])]\n",
      "target_new is : [{'boxes': tensor([[ 95,  71, 226, 313],\n",
      "        [ 72,  94, 221, 306],\n",
      "        [216,  59,  91, 126],\n",
      "        [622, 257, 564, 781]]), 'labels': tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1]])}]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "    print(\"image is : {}\".format(images.shape))\n",
    "    images_l = list(image.to('cpu') for image in images) \n",
    "    print(\"target is : {}\".format(targets))\n",
    "    d = {}\n",
    "    d['boxes'] = targets[1][:]\n",
    "    d['labels'] = targets[0][:]\n",
    "    target_new = [d]\n",
    "    targets = [{k: v.to('cpu') for k, v in t.items()} for t in target_new]\n",
    "    print(\"image_new is : {}\".format(images_l))\n",
    "    print(\"target_new is : {}\".format(targets))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mon 02:28:52,901 INF Model fasterrcnn_resnet50_fpn loaded from torchvision.models.                                                   \n"
     ]
    }
   ],
   "source": [
    "from mb_pytorch.models.modelloader import ModelLoader\n",
    "model_data_load = ModelLoader(k.data_dict['model'])\n",
    "model =  model_data_load.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(\u001b[43mimages_l\u001b[49m,targets)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images_l' is not defined"
     ]
    }
   ],
   "source": [
    "model(images_l,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_pytorch.detection.training import detection_train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mon 02:36:53,143 INF Training loop Starting                                                                                          \n",
      "Mon 02:36:53,621 INF Model fasterrcnn_resnet50_fpn loaded from torchvision.models.                                                   \n",
      "Mon 02:36:53,623 INF Model Loaded                                                                                                    \n",
      "Mon 02:36:53,625 INF Data file: CelebA loading from torchvision.datasets.                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mon 02:37:10,351 INF Optimizer and Scheduler Loaded                                                                                  \n",
      "Mon 02:37:10,353 INF Loss: <function cross_entropy at 0x7afc095c0fe0>                                                                \n",
      "Mon 02:37:10,354 INF Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: [0.9, 0.999]\n",
      "    capturable: False\n",
      "    differe\n",
      "Mon 02:37:10,355 INF Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x7afbf5503190>                                           \n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Mon 02:37:10,358 INF Training Started                                                                                                \n",
      "Mon 02:37:10,490 Mon 02:37:10,490 Mon 02:37:10,491 INF INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "\n",
      "Mon 02:37:10,502 Mon 02:37:10,502 INF Mon 02:37:10,503 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF INF \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Mon 02:37:10,511 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FMon 02:37:10,511 \n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,516 INF Mon 02:37:10,516 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,520 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,523 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,529 INF Mon 02:37:10,530 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FMon 02:37:10,531 \n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FMon 02:37:10,532 \n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,541 Mon 02:37:10,541 INF INF Mon 02:37:10,543 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF Mon 02:37:10,543 \n",
      "\n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,553 Mon 02:37:10,554 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FMon 02:37:10,555 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "INF Mon 02:37:10,556 \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Mon 02:37:10,567 INF Mon 02:37:10,568 Mon 02:37:10,568 Mon 02:37:10,569 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF INF INF \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "\n",
      "Mon 02:37:10,580 INF Mon 02:37:10,581 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Mon 02:37:10,584 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,591 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,605 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,613 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,623 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Mon 02:37:10,631 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetection_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py:69\u001b[0m, in \u001b[0;36mdetection_train_loop\u001b[0;34m(k_yaml, scheduler, writer, logger, gradcam, gradcam_rgb, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m temp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m targets[\u001b[38;5;241m0\u001b[39m][:]\n\u001b[1;32m     68\u001b[0m final_list \u001b[38;5;241m=\u001b[39m [temp_dict]\n\u001b[0;32m---> 69\u001b[0m targets_final \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_list\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets_final)\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py:69\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m temp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m targets[\u001b[38;5;241m0\u001b[39m][:]\n\u001b[1;32m     68\u001b[0m final_list \u001b[38;5;241m=\u001b[39m [temp_dict]\n\u001b[0;32m---> 69\u001b[0m targets_final \u001b[38;5;241m=\u001b[39m [\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m final_list]\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets_final)\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py:69\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m temp_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m targets[\u001b[38;5;241m0\u001b[39m][:]\n\u001b[1;32m     68\u001b[0m final_list \u001b[38;5;241m=\u001b[39m [temp_dict]\n\u001b[0;32m---> 69\u001b[0m targets_final \u001b[38;5;241m=\u001b[39m [{k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m final_list]\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets_final)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "detection_train_loop(k,model_data_dict,writer=writer,logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
