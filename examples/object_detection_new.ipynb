{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test models \n",
    "import nest_asyncio\n",
    "import warnings\n",
    "from mb_utils.src.logging import logger\n",
    "\n",
    "nest_asyncio.apply()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.41'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mb_pytorch.utils.version import version\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fri 16:29:04,730 INF Read data from yaml file: [{'data': {'from_datasets': 'CelebA', 'datasets_params_train': {'root': '/home/malav/D\n",
      "Fri 16:29:04,732 INF Data folder already exists. Using existing data folder :  /home/malav/Desktop/mb_packages/mb_pytorch/data/      \n"
     ]
    }
   ],
   "source": [
    "from mb_pytorch.dataloader.loader import DataLoader\n",
    "\n",
    "k = DataLoader('../scripts/detection/object_detection.yaml',logger=logger)\n",
    "k_data = k.data_dict['data']\n",
    "model_data_dict = k.data_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mb_pytorch.training.train_params import train_helper\n",
    "model_yaml_data = k.data_dict['model']\n",
    "loss_attr,optimizer_attr,optimizer_dict,scheduler_attr,scheduler_dict = train_helper(model_yaml_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "path_logs = os.path.join(k_data['datasets_params_train']['root'], 'logs')\n",
    "writer = SummaryWriter(log_dir=path_logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sat 02:44:06,051 INF Data file: CelebA loading from torchvision.datasets.                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader,val_loader,_,_ = k.data_load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sat 02:52:55,124 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,135 INF Sat 02:52:55,135 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FSat 02:52:55,136 INF \n",
      "transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,143 Sat 02:52:55,145 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Sat 02:52:55,148 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,150 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,155 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,159 Sat 02:52:55,159 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Sat 02:52:55,162 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image is : torch.Size([4, 3, 218, 178])\n",
      "target is : [tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1]]), tensor([[ 95,  71, 226, 313],\n",
      "        [ 72,  94, 221, 306],\n",
      "        [216,  59,  91, 126],\n",
      "        [622, 257, 564, 781]])]\n",
      "target_new is : [{'boxes': tensor([[ 95,  71, 226, 313],\n",
      "        [ 72,  94, 221, 306],\n",
      "        [216,  59,  91, 126],\n",
      "        [622, 257, 564, 781]]), 'labels': tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "         1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1]])}]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sat 02:52:55,169 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INF Sat 02:52:55,170 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FINF \n",
      "Sat 02:52:55,171 transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,175 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,179 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,183 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,190 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Sat 02:52:55,201 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "    print(\"image is : {}\".format(images.shape))\n",
    "    print(\"target is : {}\".format(targets))\n",
    "    d = {}\n",
    "    d['boxes'] = targets[1][:]\n",
    "    d['labels'] = targets[0][:]\n",
    "    target_new = [d]\n",
    "    targets = [{k: v.to('cpu') for k, v in t.items()} for t in target_new]\n",
    "    print(\"target_new is : {}\".format(targets))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mdetection_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mk_yaml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogger\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradcam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgradcam_rgb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Function to train the model\n",
      "Args:\n",
      "    k_yaml: data dictionary YAML of DataLoader\n",
      "    scheduler: scheduler\n",
      "    writer: tensorboard writer\n",
      "    logger: logger\n",
      "    gradcam: gradcam layers to be visulized\n",
      "    device: default is cpu\n",
      "output:\n",
      "    None\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "from mb_pytorch.detection.training import detection_train_loop\n",
    "detection_train_loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fri 16:29:40,991 INF Model fasterrcnn_resnet50_fpn loaded from torchvision.models.                                                   \n",
      "Fri 16:29:40,993 INF Data file: CelebA loading from torchvision.datasets.                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Fri 16:29:58,424 Fri 16:29:58,425 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FFri 16:29:58,426 \n",
      "\n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,436 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,439 Fri 16:29:58,439 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Fri 16:29:58,446 Fri 16:29:58,446 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Fri 16:29:58,451 Fri 16:29:58,451 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Fri 16:29:58,456 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FFri 16:29:58,458 \n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,464 Fri 16:29:58,464 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "Fri 16:29:58,468 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,470 INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,478 Fri 16:29:58,478 Fri 16:29:58,478 INF INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,480 \n",
      "\n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,487 Fri 16:29:58,487 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,489 \n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,496 Fri 16:29:58,496 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': FFri 16:29:58,497 \n",
      "\n",
      "INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "Fri 16:29:58,507 Fri 16:29:58,508 INF INF transforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': Ftransforms: {'transform': True, 'resize': {'val': False, 'args': {'size': [256, 256]}}, 'random_crop': {'val': F\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetection_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_data_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py:64\u001b[0m, in \u001b[0;36mdetection_train_loop\u001b[0;34m(k_yaml, scheduler, writer, logger, gradcam, gradcam_rgb, device)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     63\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m---> 64\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     67\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n",
      "File \u001b[0;32m~/Desktop/mb_packages/mb_pytorch/env/lib/python3.11/site-packages/mb_pytorch/detection/training.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     63\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m---> 64\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     67\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m model(images, targets)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "detection_train_loop(k,model_data_dict,writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
