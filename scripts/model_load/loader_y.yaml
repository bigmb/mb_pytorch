##general loader yaml file to get the loader and the data.
data:
  from_datasets: 'Flowers102' ##if from_file = True, then this model from torchvision will be used
  from_file: False  ##if False, then from_datasets is used
  work_dir: /home/malav/mb_pytorch/data/
  file: /home/malav/mb_pytorch/data/csv/file_data.csv
  use_img_dir: True
  img_dir: /home/malav/mb_pytorch/data/images1/
  thresholding_pd: 0
  
train_params:
  batch_size: 32
  num_workers: 4
  pin_memory: True
  shuffle: True
  seed : 42

test_params:
  batch_size: 32
  num_workers: 4
  pin_memory: True
  shuffle: False
  seed : 42

transformation:
  transform : True
  resize:
    val : True
    args:
      size: [128, 128]
  random_crop:
    val : False
    args:
      size: [124, 124]
  random_horizontal_flip:
    val : True
    args:
      p: 0.5
  random_vertical_flip:
    val : False
    args:
       p: 0.5
  random_rotation:
    val : False
    args:
      degrees: 90
  random_grayscale:
    val : False
    args:
      p: 0.1
  random_color_jitter:
    val : False
    args:
      brightness: 0.4
      contrast: 0.4
      saturation: 0.4
      hue: 0.1
  normalize:
    val : False
    args:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]
  to_tensor:
    val : True
  
model:
  model_type: classification #classification, segmentation, detection
  load_model: False ## if False, then it will create a new model or else it will load the model from the path provided here
  model_name: U_Net #if name exists in models, then it will be used. Also,if its in torchvision.models, then it will be used.
  model_custom: mb_pytorch.models.extra_models   ##python file where the model is defined
  model_path: /home/malav/mb_pytorch/models/ #path to save the model if model_name is not in models otherise it will use the saved one
  model_optimizer: Adam #Adam, SGD, RMSprop ,any other available in torch.optim
  model_loss: cross_entropy #mse, cross_entropy, any other available in torch.nn.functional
  model_scheduler: ExponentialLR #none, StepLR, ExponentialLR ,any other available in torch.optim.lr_scheduler
  model_pretrained: False  ##if True, then it will use the pretrained model
  model_num_classes: 102 ##number of classes in the dataset
  model_initialization: None 
  model_epochs: 30

  model_train_parameters:
    Adam: 
      {lr: 0.01, betas: [0.9, 0.999], eps: 1.0e-08, weight_decay: 0, amsgrad: False}
    SGD:
      {lr: 0.01, momentum: 0.9, dampening: 0, weight_decay: 0, nesterov: False}
    RMSprop:
      {lr: 0.01, alpha: 0.99, eps: 1.0e-08, weight_decay: 0, momentum: 0, centered: False}
    ReduceLROnPlateau:
      {mode: 'min', factor: 0.1, patience: 5, min_lr: 0, threshold: 0.0001, threshold_mode: 'rel', cooldown: 0, eps: 1e-08, verbose: False}
    cross_entropy:
      {weight: None, size_average: None, ignore_index: -100, reduce: None, reduction: 'mean'}
    mse:
      {size_average: None, reduce: None, reduction: 'mean'}
    StepLR:
      {step_size: 3, gamma: 0.2}
    ExponentialLR:
      {gamma: 0.9}

  unet_parameters:
    conv_depth: 1 #default : 2
    bottleneck_conv_depth: 0 #default : None or conv_depth
    unet_depth: 4 #default : 3
    n_channels: 3 #default : 3
    n_classes: 10 #default : 1
    n_filters: 64 #default : 64
    linear_layers: 0 #default : 0
    attention: False  #default : False
    
